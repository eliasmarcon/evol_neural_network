{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "folder = \"../input/heart.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(folder)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "Weights shape: (13, 64)\n",
      "Biases shape: (64,)\n",
      "\n",
      "Layer 3:\n",
      "Weights shape: (64, 32)\n",
      "Biases shape: (32,)\n",
      "\n",
      "Layer 5:\n",
      "Weights shape: (32, 1)\n",
      "Biases shape: (1,)\n",
      "\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 1s 1ms/step - loss: 0.6734 - accuracy: 0.5132\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.6644\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5274 - accuracy: 0.8127\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5071 - accuracy: 0.8459\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4858 - accuracy: 0.8556\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.8644\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4643 - accuracy: 0.8800\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4542 - accuracy: 0.8771\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.8771\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4360 - accuracy: 0.8878\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4195 - accuracy: 0.8985\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4146 - accuracy: 0.8966\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8985\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.9015\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3847 - accuracy: 0.9005\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.9102\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.9180\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.9210\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3542 - accuracy: 0.9239\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3421 - accuracy: 0.9220\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.9288\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3226 - accuracy: 0.9356\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.9337\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3042 - accuracy: 0.9356\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3057 - accuracy: 0.9366\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.9356\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.9337\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.9405\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2888 - accuracy: 0.9376\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.9405\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.2716 - accuracy: 0.9424\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.9395\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2586 - accuracy: 0.9454\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.9444\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2566 - accuracy: 0.9385\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2570 - accuracy: 0.9424\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.9522\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.9463\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.9502\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9415\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9473\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9424\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9522\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2230 - accuracy: 0.9532\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2198 - accuracy: 0.9463\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9444\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9512\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9502\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9571\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9610\n",
      "Trained Weights and Biases:\n",
      "Layer 1:\n",
      "Weights shape: (13, 64)\n",
      "Weights values:\n",
      "[[-0.0778731  -0.10419119 -0.08534459  0.13211088 -0.03566382 -0.08170096\n",
      "  -0.08189759  0.02543799 -0.06852285 -0.16684154  0.15050104 -0.0772104\n",
      "  -0.0788726  -0.10879691 -0.02123371 -0.26783952 -0.08411553  0.01029528\n",
      "   0.06096896 -0.1532028   0.00111799  0.04091224 -0.05971231 -0.17372137\n",
      "  -0.18691973  0.08931333 -0.26833344 -0.12579979 -0.12150112  0.04838717\n",
      "  -0.00781504  0.00951998  0.17747766 -0.19256112 -0.10362619 -0.1757643\n",
      "   0.07213986 -0.11572801  0.0142611  -0.05912108 -0.23217146  0.06607193\n",
      "   0.05234804  0.06937885 -0.03529475 -0.1423402  -0.01349115  0.17798387\n",
      "  -0.046312   -0.08802179 -0.10708677 -0.15840591  0.17133151 -0.12130248\n",
      "   0.0176961  -0.1542754   0.01269249 -0.16005418 -0.14822061 -0.10629966\n",
      "   0.06494685 -0.06857921  0.18303089  0.15217508]\n",
      " [-0.08179808  0.04178898  0.20932621 -0.09253335 -0.11485588  0.01303213\n",
      "  -0.14559418 -0.1946354  -0.01342772  0.10235453 -0.07094654 -0.07641551\n",
      "  -0.11429416  0.03945849 -0.03777885 -0.09493504 -0.06911271 -0.0832329\n",
      "  -0.04467271  0.13884796 -0.03301596  0.24563259  0.02127041  0.03500724\n",
      "  -0.1522082  -0.08348916 -0.15996878  0.03762091 -0.04535964 -0.00203373\n",
      "  -0.16386579  0.01724949 -0.13840173 -0.13558169 -0.00504269 -0.12130431\n",
      "  -0.1486893   0.05685098 -0.1438443   0.2306097  -0.13292617  0.12809393\n",
      "   0.00162527  0.07291306 -0.03636786 -0.06655259 -0.03418384 -0.04391002\n",
      "   0.01639916 -0.03677798 -0.14622872  0.15839007 -0.01447724 -0.05232198\n",
      "  -0.08113752  0.0964665  -0.02943188 -0.01649831  0.15685616 -0.00258095\n",
      "  -0.02056944  0.15579823 -0.06133546 -0.08951505]\n",
      " [ 0.15674007  0.10490205 -0.23426291 -0.11932025  0.04621942 -0.04664795\n",
      "   0.07602974  0.06813277  0.03035077  0.08499472  0.02897533  0.01880085\n",
      "   0.07308738  0.00789366 -0.0674646   0.04623269 -0.05686231 -0.24028556\n",
      "   0.00669267 -0.14230235  0.01931964  0.09628127  0.10493767  0.05506649\n",
      "  -0.06818718 -0.05325679 -0.0262419  -0.09510876 -0.03198356 -0.22165416\n",
      "  -0.0874275  -0.2677273   0.03309334  0.11898147  0.14588833  0.10682327\n",
      "  -0.04950966 -0.15282468 -0.12603575 -0.24386884  0.06981988  0.01025997\n",
      "   0.00176784 -0.12125065  0.0735034   0.00960135 -0.13408634 -0.10774989\n",
      "  -0.26203436  0.08382378  0.01553125  0.02595823  0.03462065 -0.02961517\n",
      "   0.08860298 -0.20541352 -0.07324446 -0.02591588 -0.1614937   0.04260698\n",
      "   0.10440154 -0.3100491   0.00858363  0.03826204]\n",
      " [-0.09233219  0.09322201 -0.0827017  -0.01877378  0.00503538  0.13264182\n",
      "   0.05826716  0.04872553 -0.05019296  0.1308091  -0.02792003  0.03859578\n",
      "  -0.0556696   0.01063972  0.10697225  0.07678095  0.06704174 -0.1083987\n",
      "   0.06109552  0.21106118  0.07663025  0.20477611  0.02801115 -0.00891783\n",
      "   0.1215095   0.04591949  0.09749898  0.19597876  0.17998917 -0.06140754\n",
      "   0.25483054 -0.11142296 -0.19646305  0.05380635 -0.02080161  0.02013494\n",
      "  -0.01439138  0.15113392  0.10123555 -0.13481256  0.07463484 -0.05812179\n",
      "   0.03317226  0.01602818 -0.06255396  0.08545418 -0.00878889 -0.00979722\n",
      "  -0.0153686  -0.07863093  0.04887783  0.22861648 -0.03624753 -0.04134462\n",
      "  -0.11831413  0.13779415  0.07541559  0.21114577  0.18647136 -0.04532989\n",
      "   0.04846704  0.13792613  0.01013141 -0.02359395]\n",
      " [ 0.03934985  0.02589653 -0.06294214  0.01168854  0.01596974 -0.17673264\n",
      "  -0.08494049 -0.06640344  0.13445787  0.06754658 -0.04795055  0.01945843\n",
      "  -0.10989528 -0.06292998 -0.18174933  0.2133947   0.07915528 -0.01179801\n",
      "  -0.18127567 -0.01795284 -0.11234128  0.08551513  0.01911909 -0.05755974\n",
      "   0.05217526 -0.04655509  0.13432716  0.00427584 -0.09091327  0.00826139\n",
      "   0.05650825 -0.01221314  0.0100178  -0.03256399  0.10818025  0.01601554\n",
      "   0.04773875 -0.09570327  0.03894584 -0.08795214  0.14073905 -0.0538173\n",
      "  -0.10134603 -0.04643651 -0.13119698 -0.04873832  0.10292013 -0.20542304\n",
      "  -0.00559261  0.11665336 -0.00886929  0.07102939 -0.14412063  0.04773176\n",
      "  -0.05854711 -0.08954183 -0.22696848 -0.09533529  0.05321581 -0.10625561\n",
      "  -0.0956865  -0.10705635 -0.15170553 -0.04641147]\n",
      " [ 0.01442888  0.03688474 -0.08409889 -0.09237745  0.07747991  0.12702581\n",
      "  -0.051317    0.09067136  0.06804409 -0.12967826  0.02795986  0.00748376\n",
      "   0.03807426  0.10472377  0.1106384  -0.0834153  -0.01037711 -0.0371301\n",
      "   0.0540425  -0.06719829 -0.07647876 -0.141525   -0.05965061  0.07110745\n",
      "  -0.25333226 -0.05874137 -0.18224235  0.03189194  0.07382127  0.17170778\n",
      "   0.0016513   0.13652681 -0.09842362 -0.07475478 -0.06254414 -0.06804609\n",
      "  -0.10996428 -0.04936152 -0.00777458  0.0532216   0.01792103 -0.10639752\n",
      "   0.1528746   0.12562531  0.1108616   0.06392041 -0.00744767  0.03194953\n",
      "   0.2044609  -0.0326058  -0.04176104 -0.13688374  0.10145065  0.04490642\n",
      "   0.07752898 -0.0419695   0.09835029  0.12434024 -0.01505139  0.09343334\n",
      "  -0.04730547  0.00054495  0.0366412   0.01007373]\n",
      " [-0.01715409 -0.16232015 -0.18149264  0.07065868  0.10111191  0.02111565\n",
      "  -0.07339158  0.10497761 -0.02274735  0.11343043  0.03319323  0.08154754\n",
      "   0.09857795 -0.00112457  0.0330552   0.06281382 -0.01360903 -0.2578759\n",
      "  -0.00662991  0.00768272  0.16325796  0.06650233 -0.19829407 -0.00685412\n",
      "   0.12932861 -0.03286657  0.13671945 -0.08850631  0.10992523  0.04642725\n",
      "  -0.00396677  0.03023348 -0.04670611 -0.09071366 -0.07374877 -0.07403135\n",
      "   0.05416942  0.10914022  0.0120153  -0.08595105  0.06000407  0.05251667\n",
      "   0.03943686  0.02829635 -0.1370512   0.09123757 -0.03701495 -0.13064487\n",
      "   0.00809907 -0.04114332  0.03607402  0.10734888 -0.0356232   0.03285364\n",
      "   0.04095504  0.10291504 -0.00712652  0.14624453 -0.07534384  0.01262064\n",
      "  -0.1555202  -0.08612353 -0.01696132 -0.00578196]\n",
      " [ 0.12439162 -0.02138246 -0.0567832  -0.09570071  0.0994691   0.16216755\n",
      "  -0.0697144  -0.02662086 -0.07402272 -0.32080188  0.01445452  0.18819061\n",
      "   0.09285851  0.15608245  0.04004775  0.02018945  0.00953826  0.1539047\n",
      "  -0.00725486 -0.21621929 -0.11653594  0.06602345 -0.11094157  0.20925741\n",
      "  -0.10461135 -0.03230478 -0.06104906 -0.16773859  0.10172014 -0.00496137\n",
      "  -0.03280806  0.02181995 -0.16935769  0.07348574  0.11999759  0.02141124\n",
      "   0.05505723 -0.18103802  0.07967693 -0.02552331  0.00062357 -0.04214455\n",
      "   0.1207711   0.01427221  0.12821694  0.0357565  -0.03528687  0.25027856\n",
      "  -0.1511389   0.11296074  0.0714003  -0.27028012  0.00212797 -0.20237128\n",
      "  -0.00831885 -0.20167385  0.04239734  0.07652242 -0.29057458  0.0705058\n",
      "  -0.19104579 -0.11227519 -0.20643274  0.01100411]\n",
      " [ 0.22351056  0.13162223  0.09062742 -0.20046477  0.15720668  0.02326831\n",
      "  -0.0671684   0.07022047 -0.06191849 -0.04523148  0.03715656  0.04506435\n",
      "  -0.1056566   0.03970956 -0.21551682 -0.06572977 -0.13666481 -0.0941629\n",
      "   0.05193593  0.068762    0.11699633 -0.03815659  0.06115735  0.17928311\n",
      "  -0.19103472 -0.2652208  -0.07014138 -0.03853999 -0.11296226  0.17615378\n",
      "  -0.04103108  0.1298334   0.15672453 -0.08430467  0.2556772  -0.01172273\n",
      "  -0.01774376  0.09567716  0.04166888  0.10069306 -0.0338677  -0.06624918\n",
      "   0.07837002  0.19260733  0.15518568 -0.2025212  -0.1543927   0.14751862\n",
      "   0.19944286  0.19801155 -0.1139881   0.01566829  0.13692716 -0.03670631\n",
      "   0.03889501  0.11542089 -0.10309777 -0.09583816  0.01369642  0.02626912\n",
      "  -0.04776709 -0.05712989 -0.18455255  0.06950545]\n",
      " [ 0.11436473 -0.12253403  0.29936254  0.11320956  0.04384293 -0.0545658\n",
      "   0.03451115 -0.02128334 -0.09132942 -0.1073864  -0.01712101  0.0228383\n",
      "  -0.05305162 -0.05407232  0.02459094 -0.04197486  0.0247594   0.04971104\n",
      "  -0.03952988  0.19589955  0.21341114  0.18515345 -0.0647283  -0.03219597\n",
      "   0.07606546  0.09582903  0.04177301  0.07191759  0.05882481  0.03588412\n",
      "  -0.30807152  0.08960618  0.07058096 -0.15088797 -0.01931994 -0.03885934\n",
      "   0.02624919  0.05472631 -0.08849445  0.31127965  0.01555456  0.3139687\n",
      "   0.01628384  0.25329024 -0.04572525 -0.05433592  0.05013748  0.25917438\n",
      "   0.07522936  0.03495254  0.0287615  -0.00896874 -0.0792714  -0.25996578\n",
      "  -0.01142106  0.06801462  0.03402467 -0.08700636 -0.05725832 -0.04338291\n",
      "  -0.01367718 -0.02439033  0.03106521  0.00721282]\n",
      " [ 0.13428769 -0.09404221 -0.06830882  0.16783458  0.23366594  0.16463143\n",
      "  -0.06109561  0.2051846  -0.2609278  -0.04701022  0.14556733  0.08729795\n",
      "   0.02384468  0.02621565  0.00182982 -0.09803525 -0.04512374 -0.00787762\n",
      "   0.02677662 -0.08442054 -0.10405803 -0.04085803 -0.11568601  0.13899021\n",
      "  -0.08554183 -0.00906175 -0.05336185  0.03105089  0.03024149 -0.0985653\n",
      "  -0.06205821 -0.02301936  0.13100266 -0.22865382 -0.05491027 -0.12933539\n",
      "   0.08335163 -0.13705307 -0.14241344 -0.07738759  0.03393162 -0.09453783\n",
      "   0.21557558 -0.15494378  0.12831977 -0.12283877 -0.02962414 -0.1602306\n",
      "  -0.07366031  0.06072606 -0.01566172 -0.07052127  0.10158014  0.07115404\n",
      "   0.07025795 -0.09879395  0.02481924 -0.05346913  0.00259072  0.04349321\n",
      "  -0.07625387 -0.07474609  0.03595891  0.17308335]\n",
      " [-0.11820816 -0.2352527   0.1672299  -0.03302954  0.09724304 -0.15373829\n",
      "  -0.02133797  0.12681285 -0.23854053 -0.03421091 -0.2232594  -0.01177856\n",
      "  -0.09903555 -0.16088727 -0.26024255  0.19927225 -0.19350061  0.10090499\n",
      "  -0.2707332   0.04459769  0.12207326  0.01589126 -0.23591736 -0.17081569\n",
      "   0.07603168 -0.13046221  0.16778746  0.06368997 -0.14689794  0.10645195\n",
      "  -0.04305971  0.13779771  0.03109125  0.04775942 -0.15657696  0.00805084\n",
      "  -0.10889644  0.04421033 -0.08097289  0.1613501   0.12329226  0.12876643\n",
      "  -0.16452274  0.11593088 -0.14718752 -0.14450616 -0.1961795   0.09212659\n",
      "   0.11938845  0.07926576 -0.06184912 -0.02011673 -0.25272843 -0.06587526\n",
      "  -0.15129352  0.04641213 -0.31856826 -0.07039617  0.10889107 -0.270739\n",
      "  -0.3079105   0.1700078  -0.13229705 -0.21470708]\n",
      " [ 0.11146995 -0.02964844  0.14964733  0.03567912  0.10168605  0.01648644\n",
      "  -0.06646661  0.0357868   0.05438213  0.05901776  0.04069145  0.02101457\n",
      "   0.02946159  0.04623451 -0.02464059 -0.1591672  -0.0563867   0.09808021\n",
      "   0.01167621  0.2672543   0.06999481  0.11818221 -0.05410664  0.00966109\n",
      "  -0.25221932 -0.08012661 -0.18900387  0.18882903 -0.06654722 -0.12390213\n",
      "   0.14978473 -0.06684741  0.08378808 -0.08319908  0.11121855 -0.02075434\n",
      "  -0.00164938  0.3717149   0.17215958  0.0271989  -0.09330756  0.08799625\n",
      "   0.14163902  0.00040492  0.11686456 -0.02468068 -0.03081589 -0.01952126\n",
      "  -0.03752722  0.11597657 -0.07561024  0.17405044  0.16158822 -0.09138817\n",
      "   0.10724942  0.3663845   0.00961767 -0.00408166  0.18316408  0.08135858\n",
      "  -0.04330771  0.24129005 -0.00843156  0.04335785]]\n",
      "Biases shape: (64,)\n",
      "Biases values:\n",
      "[ 0.00092906 -0.07076898 -0.03776892 -0.03474949 -0.02448765 -0.05094852\n",
      "  0.00612954 -0.01975211 -0.11822827 -0.10991155  0.0190809  -0.02640267\n",
      " -0.00123547  0.01425208 -0.11281422 -0.12515874 -0.01412188 -0.10735132\n",
      " -0.04005663  0.01247223  0.01707551 -0.08722734 -0.11143995  0.0035989\n",
      " -0.09922765  0.0016548  -0.08550641  0.02372019 -0.02626216 -0.16794223\n",
      " -0.07406886 -0.0486109   0.06358058 -0.06974186 -0.06132274 -0.03552573\n",
      "  0.05467236 -0.02028742 -0.07545351 -0.01637119 -0.11479512 -0.05930699\n",
      " -0.06415623  0.07099606 -0.07697071 -0.04029942 -0.01654978 -0.00194463\n",
      " -0.07927489  0.02149949 -0.02750788 -0.10582291 -0.05600964 -0.06908633\n",
      " -0.00099247  0.00376309 -0.0683056  -0.03547834 -0.06231427  0.03428276\n",
      " -0.08156956 -0.03057853 -0.03930355  0.032828  ]\n",
      "\n",
      "Layer 3:\n",
      "Weights shape: (64, 32)\n",
      "Weights values:\n",
      "[[ 0.0930007   0.15429819  0.07001548 ...  0.06090901  0.14351735\n",
      "   0.1227345 ]\n",
      " [ 0.10562494  0.077719    0.08976286 ... -0.01173058  0.10711791\n",
      "   0.12758607]\n",
      " [-0.36464384 -0.32436055 -0.3258307  ... -0.05118895 -0.39499098\n",
      "  -0.39336383]\n",
      " ...\n",
      " [-0.27707446 -0.26782307 -0.25397676 ...  0.00909068 -0.22212069\n",
      "  -0.24835618]\n",
      " [ 0.12222854  0.12439064  0.1768981  ...  0.08122219  0.1312949\n",
      "   0.1672473 ]\n",
      " [ 0.06923781  0.13616416  0.15354374 ...  0.01775605  0.10005027\n",
      "   0.09023665]]\n",
      "Biases shape: (32,)\n",
      "Biases values:\n",
      "[-0.02070624 -0.01756473 -0.00306982 -0.05597001 -0.02143493 -0.03707553\n",
      " -0.02889697 -0.0099829  -0.03796506 -0.01878498 -0.00725829  0.0148881\n",
      " -0.01126156 -0.01645803 -0.01311139 -0.02249431 -0.01334632 -0.02082093\n",
      " -0.01568311 -0.03823844 -0.01071357  0.00391642 -0.01735937 -0.02629411\n",
      " -0.02236395 -0.01387504 -0.02456726 -0.01437861 -0.02444573 -0.02244331\n",
      " -0.01925244 -0.01397222]\n",
      "\n",
      "Layer 5:\n",
      "Weights shape: (32, 1)\n",
      "Weights values:\n",
      "[[0.3734346 ]\n",
      " [0.368259  ]\n",
      " [0.3473513 ]\n",
      " [0.32969165]\n",
      " [0.41832587]\n",
      " [0.40848905]\n",
      " [0.4454564 ]\n",
      " [0.3836881 ]\n",
      " [0.48855132]\n",
      " [0.43658444]\n",
      " [0.3768908 ]\n",
      " [0.40123773]\n",
      " [0.2484704 ]\n",
      " [0.34736735]\n",
      " [0.41220662]\n",
      " [0.3464417 ]\n",
      " [0.3763459 ]\n",
      " [0.3763174 ]\n",
      " [0.38423175]\n",
      " [0.36118796]\n",
      " [0.3791579 ]\n",
      " [0.45821947]\n",
      " [0.38186738]\n",
      " [0.36352873]\n",
      " [0.38076803]\n",
      " [0.6331355 ]\n",
      " [0.42058057]\n",
      " [0.4049578 ]\n",
      " [0.39420047]\n",
      " [0.09811565]\n",
      " [0.36166954]\n",
      " [0.42721623]]\n",
      "Biases shape: (1,)\n",
      "Biases values:\n",
      "[-1.2639875]\n",
      "\n",
      "Ranges of Trained Weights:\n",
      "Layer 1 weight range: (-0.32080188, 0.3717149)\n",
      "Layer 3 weight range: (-0.4632043, 0.33242083)\n",
      "Layer 5 weight range: (0.098115645, 0.6331355)\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.1786 - accuracy: 0.9649\n",
      "Test Accuracy: 0.9648780226707458\n"
     ]
    }
   ],
   "source": [
    "# Define constant arrays for weight initialization using RandomNormal\n",
    "mean = 0\n",
    "stddev = 0.05\n",
    "\n",
    "big_array_layer1 = np.random.normal(loc=mean, scale=stddev, size=(13, 64))\n",
    "big_array_layer2 = np.random.normal(loc=mean, scale=stddev, size=(64, 32))\n",
    "big_array_output = np.random.normal(loc=mean, scale=stddev, size=1)\n",
    "\n",
    "# Build the neural network model with weight initialization from the big arrays\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_initializer=Constant(value=big_array_layer1)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu', kernel_initializer=Constant(value=big_array_layer2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=Constant(value=big_array_output)))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the shapes of the weights and biases\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if i % 2 == 0:\n",
    "        weights, biases = layer.get_weights()\n",
    "        print(f\"Layer {i + 1}:\")\n",
    "        print(f\"Weights shape: {weights.shape}\")\n",
    "        print(f\"Biases shape: {biases.shape}\")\n",
    "        print()\n",
    "        \n",
    "    \n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y, epochs=50, batch_size=32)\n",
    "\n",
    "# Print the shapes of the weights and biases after training\n",
    "print(\"Trained Weights and Biases:\")\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if i % 2 == 0:\n",
    "        weights, biases = layer.get_weights()\n",
    "        print(f\"Layer {i + 1}:\")\n",
    "        print(f\"Weights shape: {weights.shape}\")\n",
    "        print(f\"Weights values:\\n{weights}\")\n",
    "        print(f\"Biases shape: {biases.shape}\")\n",
    "        print(f\"Biases values:\\n{biases}\")\n",
    "        print()\n",
    "\n",
    "# Print the ranges of the trained weights after training\n",
    "print(\"Ranges of Trained Weights:\")\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if i % 2 == 0:\n",
    "        weights, _ = layer.get_weights()\n",
    "        weight_range = (np.min(weights), np.max(weights))\n",
    "        print(f\"Layer {i + 1} weight range: {weight_range}\")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_train_scaled, y)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
